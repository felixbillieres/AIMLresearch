# 02 â€” LLM & Transformers

> **Phase:** 1 | **Timeline:** Aprilâ€“May 2026 | **Status:** â¬œ Not Started

## Objective
Deeply understand how LLMs work â€” from the Transformer architecture to practical model manipulation with HuggingFace. Implement a mini-transformer from scratch.

## Curriculum

### HuggingFace NLP Course
- ðŸ”— https://huggingface.co/learn/nlp-course
- **Target:** Chapters 1â€“4 (Pipeline, Tokenizers, Fine-tuning, Sharing)

### Transformer Deep Dive
- Paper: "Attention Is All You Need" (Vaswani et al.)
- Blog: Jay Alammar "The Illustrated Transformer"
- Blog: Lilian Weng "Attention? Attention!"

### Build: Mini-Transformer from Scratch
- Use Karpathy's code as a starting point
- Implement self-attention, multi-head attention, positional encoding
- Train on a small text dataset

## Success Criteria
- [ ] Can explain self-attention mechanism from memory
- [ ] HuggingFace course chapters 1â€“4 completed
- [ ] Mini-transformer implemented and training
- [ ] Can fine-tune a small model on a custom dataset
